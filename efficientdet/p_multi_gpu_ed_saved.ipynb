{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 16:16:16.690818: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-10 16:16:17.620984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22345 MB memory:  -> device: 0, name: Graphics Device, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-02-10 16:16:17.622143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22345 MB memory:  -> device: 1, name: Graphics Device, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gmpark/anaconda3/envs/ed/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export 생략하고 inference\n",
    "#### 사전 export된 모델을 각 gpu로 읽어와서 실행 가능하기 때문에, export과정은 gpu mapping 필요 없음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default flag setting\n",
    "\n",
    "flag = {}\n",
    "\n",
    "flag['model_name'] = 'efficientdet-d0'\n",
    "flag['logdir'] = '/tmp/deff/'\n",
    "flag['runmode'] = 'dry'\n",
    "flag['trace_filename'] = None\n",
    "\n",
    "flag['threads'] = 0\n",
    "flag['bm_runs'] = 10\n",
    "flag['tensorrt'] = None\n",
    "flag['delete_logdir'] =True\n",
    "flag['freeze'] = False\n",
    "flag['use_xla'] =False\n",
    "flag['batch_size'] = 1\n",
    "\n",
    "flag['ckpt_path'] = None\n",
    "flag['export_ckpt'] = None\n",
    "\n",
    "flag['hparams'] = ''\n",
    "flag['input_image'] = None\n",
    "flag['output_image_dir'] = None\n",
    "\n",
    "flag['input_video'] = None\n",
    "flag['output_video'] = None\n",
    "\n",
    "flag['line_thickness'] = None\n",
    "flag['max_boxes_to_draw'] = 100\n",
    "flag['min_score_thresh'] = 0.4\n",
    "flag['nms_method'] = 'hard'\n",
    "\n",
    "flag['saved_model_dir'] = '/tmp/saved_model'\n",
    "flag['tfile_path'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use model in /home/gmpark/automl/efficientdet/efficientdet-d0\n"
     ]
    }
   ],
   "source": [
    "# export parameter\n",
    "\n",
    "MODEL = 'efficientdet-d0'  #@param\n",
    "\n",
    "def download(m):\n",
    "  if m not in os.listdir():\n",
    "    !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/{m}.tar.gz\n",
    "    !tar zxf {m}.tar.gz\n",
    "  ckpt_path = os.path.join(os.getcwd(), m)\n",
    "  return ckpt_path\n",
    "\n",
    "# Download checkpoint.\n",
    "ckpt_path = download(MODEL)\n",
    "print('Use model in {}'.format(ckpt_path))\n",
    "\n",
    "\n",
    "flag['runmode'] = 'saved_model'\n",
    "flag['model_name'] = 'efficientdet-d0'\n",
    "flag['ckpt_path'] = ckpt_path\n",
    "flag['hparams'] = 'image_size=1920x1280'\n",
    "flag['saved_model_dir'] = 'savedmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference parameter\n",
    "\n",
    "flag['runmode'] = 'saved_model_infer'\n",
    "flag['model_name'] = 'efficientdet-d0'\n",
    "flag['saved_model_dir'] = 'savedmodel'\n",
    "flag['input_image'] = 'img.png'\n",
    "flag['output_image_dir'] = 'serve_image_out'\n",
    "flag['min_score_thresh'] = 0.35\n",
    "flag['max_boxes_to_draw'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_mdoel_inspect...\n",
      "WARNING:tensorflow:From /tmp/ipykernel_27113/2744426831.py:31: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-10 16:16:19.387316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22345 MB memory:  -> device: 0, name: Graphics Device, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-02-10 16:16:19.388156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22345 MB memory:  -> device: 1, name: Graphics Device, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from savedmodel/variables/variables\n"
     ]
    }
   ],
   "source": [
    "from p_model_inspect import P_ModelInspector\n",
    "from PIL import Image\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "  tf.disable_eager_execution()\n",
    "  inspector = P_ModelInspector(\n",
    "    model_name=flag['model_name'],\n",
    "    logdir=flag['logdir'],\n",
    "    tensorrt=flag['tensorrt'],\n",
    "    use_xla=flag['use_xla'],\n",
    "    ckpt_path=flag['ckpt_path'],\n",
    "    export_ckpt=flag['export_ckpt'],\n",
    "    saved_model_dir=flag['saved_model_dir'],\n",
    "    tflite_path=flag['tfile_path'],\n",
    "    batch_size=flag['batch_size'],\n",
    "    hparams=flag['hparams'],\n",
    "    score_thresh=flag['min_score_thresh'],\n",
    "    max_output_size=flag['max_boxes_to_draw'],\n",
    "    nms_method=flag['nms_method'])\n",
    "  \n",
    "  \n",
    "  import tensorflow.compat.v1 as tf\n",
    "  sess_config = tf.ConfigProto()\n",
    "  sess = tf.Session(config=sess_config)\n",
    "  signitures={\n",
    "        'image_files': 'image_files:0',\n",
    "        'image_arrays': 'image_arrays:0',\n",
    "        'prediction': 'detections:0',}\n",
    "  if tf.io.gfile.isdir(flag['saved_model_dir']):\n",
    "    tf.saved_model.load(sess, ['serve'],\n",
    "                                 flag['saved_model_dir'])\n",
    "  \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_model_inspect import P_ModelInspector\n",
    "from PIL import Image\n",
    "\n",
    "def parameter_step():\n",
    "  '''\n",
    "  driver = inspector.run_model(\n",
    "      flag['runmode'],\n",
    "      input_image=flag['input_image'],\n",
    "      output_image_dir=flag['output_image_dir'],\n",
    "      input_video=flag['input_video'],\n",
    "      output_video=flag['output_video'],\n",
    "      line_thickness=flag['line_thickness'],\n",
    "      max_boxes_to_draw=flag['max_boxes_to_draw'],\n",
    "      min_score_thresh=flag['min_score_thresh'],\n",
    "      nms_method=flag['nms_method'],\n",
    "      bm_runs=flag['bm_runs'],\n",
    "      threads=flag['threads'],\n",
    "      trace_filename=flag['trace_filename'])\n",
    "    # Serving time batch size should be fixed.\n",
    "    '''\n",
    "    \n",
    "  \n",
    "  batch_size = flag['batch_size'] or 1\n",
    "  all_files = list(tf.io.gfile.glob(flag['input_image']))\n",
    "  print('all_files=', all_files)\n",
    "  num_batches = (len(all_files) + batch_size - 1) // batch_size\n",
    "\n",
    "  for i in range(num_batches):\n",
    "    batch_files = all_files[i * batch_size:(i + 1) * batch_size]\n",
    "    height, width = (1280, 1920) # 직접입력\n",
    "    images = [Image.open(f) for f in batch_files]\n",
    "    if len(set([m.size for m in images])) > 1:\n",
    "      # Resize only if images in the same batch have different sizes.\n",
    "      images = [m.resize(height, width) for m in images]\n",
    "    raw_images = [np.array(m) for m in images]\n",
    "    size_before_pad = len(raw_images)\n",
    "    if size_before_pad < batch_size:\n",
    "      padding_size = batch_size - size_before_pad\n",
    "      raw_images += [np.zeros_like(raw_images[0])] * padding_size\n",
    "    \n",
    "    #serve image\n",
    "    detections_bs = sess.run(signitures['prediction'],feed_dict={signitures['image_arrays']:raw_images})\n",
    "    \n",
    "    for j in range(size_before_pad):\n",
    "      #img = driver.visualize(raw_images[j], detections_bs[j], max_boxes_to_draw=200,min_score_thresh=0.35)\n",
    "      \n",
    "      #visualize_image_prediction(image, prediction,label_map=self.label_map,**kwargs)\n",
    "      \n",
    "      boxes = detections_bs[j][:, 1:5]\n",
    "      classes = detections_bs[j][:, 6].astype(int)\n",
    "      scores = detections_bs[j][:, 5]\n",
    "      from tf2 import label_util\n",
    "      from visualize import vis_utils\n",
    "      '''\n",
    "      #visualize_image(raw_images[j],boxes,classes,scores,None,max_boxes_to_draw=200,min_score_thresh=0.35)\n",
    "      label_map = label_util.get_label_map(None or 'coco')\n",
    "      category_index = {k: {'id': k, 'name': label_map[k]} for k in label_map}\n",
    "      img = np.array(raw_images[j])\n",
    "      vis_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      img,\n",
    "      boxes,\n",
    "      classes,\n",
    "      scores,\n",
    "      category_index,\n",
    "      min_score_thresh=flag['min_score_thresh'],\n",
    "      max_boxes_to_draw=flag['max_boxes_to_draw'],\n",
    "      line_thickness=flag['line_thickness'],\n",
    "      )\n",
    "      \n",
    "      \n",
    "      img_id = str(i * batch_size + j)\n",
    "      output_image_path = os.path.join(flag['output_image_dir'], img_id + '.jpg')\n",
    "      Image.fromarray(img).save(output_image_path)\n",
    "      print('writing file to %s' % output_image_path)\n",
    "      '''\n",
    "  print('run')\n",
    "\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_files= ['img.png']\n",
      "run\n",
      "all_files= ['img.png']\n",
      "run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'PartitionedCall_2:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model export\n",
    "@tf.function\n",
    "\n",
    "def distributed_run_step():\n",
    "    per_replica = mirrored_strategy.run(parameter_step)\n",
    "    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica,axis=None)\n",
    "distributed_run_step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parameter_step():\n",
    "  #parameter setting\n",
    "  runmode='saved_model_infer'\n",
    "  model_name=MODEL\n",
    "  saved_model_dir = 'savedmodel'\n",
    "  input_image='img.png'\n",
    "  serve_image_out = 'serve_image_out'\n",
    "  output_image_dir='serve_image_out'\n",
    "  image_dir=serve_image_out\n",
    "  max_boxes_to_draw=100\n",
    "  min_score_thresh=0.4\n",
    "  batch_size=1\n",
    "  min_score_thresh=min_score_thresh\n",
    "  max_boxes_to_draw=max_boxes_to_draw\n",
    "\n",
    "\n",
    "  \n",
    "  driver = inspector.run_model(\n",
    "    runmode,\n",
    "    input_image=input_image,\n",
    "    output_image_dir=output_image_dir,\n",
    "    input_video=input_video,\n",
    "    output_video=output_video,\n",
    "    line_thickness=line_thickness,\n",
    "    max_boxes_to_draw=max_boxes_to_draw,\n",
    "    min_score_thresh=min_score_thresh,\n",
    "    nms_method='hard',\n",
    "    bm_runs=10,\n",
    "    threads=0,\n",
    "    trace_filename=None)\n",
    "  \n",
    "    \n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model export\n",
    "@tf.function\n",
    "def distributed_run_step():\n",
    "    per_replica = mirrored_strategy.run(parameter_step)\n",
    "    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica,axis=None)\n",
    "results = distributed_run_step()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://hpkim0512.blogspot.com/2017/11/tensorflow-multi-gpu.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3b316d9bafbceebb8a7c74076820eeaf33b51a50a55a86f48ec9f9d66c01e89"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('ed': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
