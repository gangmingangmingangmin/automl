{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 19:49:27.424529: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-07 19:49:28.697267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22345 MB memory:  -> device: 0, name: Graphics Device, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-02-07 19:49:28.698369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22345 MB memory:  -> device: 1, name: Graphics Device, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\",\"/gpu:1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gmpark/anaconda3/envs/ed/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export 생략하고 inference\n",
    "#### 사전 export된 모델을 각 gpu로 읽어와서 실행 가능하기 때문에, export과정은 gpu mapping 필요 없음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default flag setting\n",
    "\n",
    "flag = {}\n",
    "\n",
    "flag['model_name'] = 'efficientdet-d0'\n",
    "flag['logdir'] = '/tmp/deff/'\n",
    "flag['runmode'] = 'dry'\n",
    "flag['trace_filename'] = None\n",
    "\n",
    "flag['threads'] = 0\n",
    "flag['bm_runs'] = 10\n",
    "flag['tensorrt'] = None\n",
    "flag['delete_logdir'] =True\n",
    "flag['freeze'] = False\n",
    "flag['use_xla'] =False\n",
    "flag['batch_size'] = 1\n",
    "\n",
    "flag['ckpt_path'] = None\n",
    "flag['export_ckpt'] = None\n",
    "\n",
    "flag['hparams'] = ''\n",
    "flag['input_image'] = None\n",
    "flag['output_image_dir'] = None\n",
    "\n",
    "flag['input_video'] = None\n",
    "flag['output_video'] = None\n",
    "\n",
    "flag['line_thickness'] = None\n",
    "flag['max_boxes_to_draw'] = 100\n",
    "flag['min_score_thresh'] = 0.4\n",
    "flag['nms_method'] = 'hard'\n",
    "\n",
    "flag['saved_model_dir'] = '/tmp/saved_model'\n",
    "flag['tfile_path'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use model in /home/gmpark/automl/efficientdet/efficientdet-d0\n"
     ]
    }
   ],
   "source": [
    "# export parameter\n",
    "\n",
    "MODEL = 'efficientdet-d0'  #@param\n",
    "\n",
    "def download(m):\n",
    "  if m not in os.listdir():\n",
    "    !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/{m}.tar.gz\n",
    "    !tar zxf {m}.tar.gz\n",
    "  ckpt_path = os.path.join(os.getcwd(), m)\n",
    "  return ckpt_path\n",
    "\n",
    "# Download checkpoint.\n",
    "ckpt_path = download(MODEL)\n",
    "print('Use model in {}'.format(ckpt_path))\n",
    "\n",
    "\n",
    "flag['runmode'] = 'saved_model'\n",
    "flag['model_name'] = 'efficientdet-d0'\n",
    "flag['ckpt_path'] = ckpt_path\n",
    "flag['hparams'] = 'image_size=1920x1280'\n",
    "flag['saved_model_dir'] = 'savedmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference parameter\n",
    "\n",
    "flag['runmode'] = 'saved_model_infer'\n",
    "flag['model_name'] = 'efficientdet-d0'\n",
    "flag['saved_model_dir'] = 'savedmodel'\n",
    "flag['input_image'] = 'img.png'\n",
    "flag['output_image_dir'] = 'serve_image_out'\n",
    "flag['min_score_thresh'] = 0.35\n",
    "flag['max_boxes_to_draw'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_mdoel_inspect...\n",
      "run_model\n",
      "WARNING:tensorflow:From /home/gmpark/automl/efficientdet/inference.py:574: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 19:49:32.736715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22345 MB memory:  -> device: 0, name: Graphics Device, pci bus id: 0000:17:00.0, compute capability: 8.6\n",
      "2022-02-07 19:49:32.737540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22345 MB memory:  -> device: 1, name: Graphics Device, pci bus id: 0000:65:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from savedmodel/variables/variables\n"
     ]
    }
   ],
   "source": [
    "from p_model_inspect import P_ModelInspector\n",
    "with mirrored_strategy.scope():\n",
    "  inspector = P_ModelInspector(\n",
    "    model_name=flag['model_name'],\n",
    "    logdir=flag['logdir'],\n",
    "    tensorrt=flag['tensorrt'],\n",
    "    use_xla=flag['use_xla'],\n",
    "    ckpt_path=flag['ckpt_path'],\n",
    "    export_ckpt=flag['export_ckpt'],\n",
    "    saved_model_dir=flag['saved_model_dir'],\n",
    "    tflite_path=flag['tfile_path'],\n",
    "    batch_size=flag['batch_size'],\n",
    "    hparams=flag['hparams'],\n",
    "    score_thresh=flag['min_score_thresh'],\n",
    "    max_output_size=flag['max_boxes_to_draw'],\n",
    "    nms_method=flag['nms_method'])\n",
    "  \n",
    "  driver = inspector.run_model(\n",
    "      flag['runmode'],\n",
    "      input_image=flag['input_image'],\n",
    "      output_image_dir=flag['output_image_dir'],\n",
    "      input_video=flag['input_video'],\n",
    "      output_video=flag['output_video'],\n",
    "      line_thickness=flag['line_thickness'],\n",
    "      max_boxes_to_draw=flag['max_boxes_to_draw'],\n",
    "      min_score_thresh=flag['min_score_thresh'],\n",
    "      nms_method=flag['nms_method'],\n",
    "      bm_runs=flag['bm_runs'],\n",
    "      threads=flag['threads'],\n",
    "      trace_filename=flag['trace_filename'])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from p_model_inspect import P_ModelInspector\n",
    "from PIL import Image\n",
    "\n",
    "def parameter_step():\n",
    "  print('run')\n",
    "  # Serving time batch size should be fixed.\n",
    "  batch_size = flag['batch_size'] or 1\n",
    "  all_files = list(tf.io.gfile.glob(flag['input_image']))\n",
    "  print('all_files=', all_files)\n",
    "  num_batches = (len(all_files) + batch_size - 1) // batch_size\n",
    "\n",
    "  for i in range(num_batches):\n",
    "    batch_files = all_files[i * batch_size:(i + 1) * batch_size]\n",
    "    height, width = (1280, 1920) # 직접입력\n",
    "    images = [Image.open(f) for f in batch_files]\n",
    "    if len(set([m.size for m in images])) > 1:\n",
    "      # Resize only if images in the same batch have different sizes.\n",
    "      images = [m.resize(height, width) for m in images]\n",
    "    raw_images = [np.array(m) for m in images]\n",
    "    size_before_pad = len(raw_images)\n",
    "    if size_before_pad < batch_size:\n",
    "      padding_size = batch_size - size_before_pad\n",
    "      raw_images += [np.zeros_like(raw_images[0])] * padding_size\n",
    "\n",
    "    detections_bs = driver.serve_images(raw_images)\n",
    "    for j in range(size_before_pad):\n",
    "      img = driver.visualize(raw_images[j], detections_bs[j], max_boxes_to_draw=200,min_score_thresh=0.35)\n",
    "      img_id = str(i * batch_size + j)\n",
    "      output_image_path = os.path.join(flag['output_image_dir'], img_id + '.jpg')\n",
    "      Image.fromarray(img).save(output_image_path)\n",
    "      print('writing file to %s' % output_image_path)\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "all_files= ['img.png']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-07 19:49:44.237478: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8200\n",
      "2022-02-07 19:49:45.639843: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing file to serve_image_out/0.jpg\n",
      "run\n",
      "all_files= ['img.png']\n",
      "writing file to serve_image_out/0.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'PartitionedCall:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model export\n",
    "@tf.function\n",
    "\n",
    "def distributed_run_step():\n",
    "    per_replica = mirrored_strategy.run(parameter_step)\n",
    "    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica,axis=None)\n",
    "distributed_run_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parameter_step():\n",
    "  #parameter setting\n",
    "  runmode='saved_model_infer'\n",
    "  model_name=MODEL\n",
    "  saved_model_dir = 'savedmodel'\n",
    "  input_image='img.png'\n",
    "  serve_image_out = 'serve_image_out'\n",
    "  output_image_dir='serve_image_out'\n",
    "  image_dir=serve_image_out\n",
    "  max_boxes_to_draw=100\n",
    "  min_score_thresh=0.4\n",
    "  batch_size=1\n",
    "  min_score_thresh=min_score_thresh\n",
    "  max_boxes_to_draw=max_boxes_to_draw\n",
    "\n",
    "\n",
    "  \n",
    "  driver = inspector.run_model(\n",
    "    runmode,\n",
    "    input_image=input_image,\n",
    "    output_image_dir=output_image_dir,\n",
    "    input_video=input_video,\n",
    "    output_video=output_video,\n",
    "    line_thickness=line_thickness,\n",
    "    max_boxes_to_draw=max_boxes_to_draw,\n",
    "    min_score_thresh=min_score_thresh,\n",
    "    nms_method='hard',\n",
    "    bm_runs=10,\n",
    "    threads=0,\n",
    "    trace_filename=None)\n",
    "  \n",
    "    \n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    File \"/tmp/ipykernel_17020/1658758359.py\", line 18, in parameter_step  *\n",
      "        driver = inspector.run_model(\n",
      "\n",
      "    NameError: name 'input_video' is not defined\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gmpark/anaconda3/envs/ed/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/gmpark/anaconda3/envs/ed/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/home/gmpark/anaconda3/envs/ed/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 699, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "NameError: in user code:\n",
      "\n",
      "    File \"/tmp/ipykernel_17020/1658758359.py\", line 18, in parameter_step  *\n",
      "        driver = inspector.run_model(\n",
      "\n",
      "    NameError: name 'input_video' is not defined\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_17020/2026198366.py\", line 4, in distributed_run_step  *\n        per_replica = mirrored_strategy.run(parameter_step)\n    File \"/tmp/ipykernel_17020/1658758359.py\", line 18, in parameter_step  *\n        driver = inspector.run_model(\n\n    NameError: name 'input_video' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17020/2026198366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mper_replica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmirrored_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmirrored_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mper_replica\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_run_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ed/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ed/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_17020/2026198366.py\", line 4, in distributed_run_step  *\n        per_replica = mirrored_strategy.run(parameter_step)\n    File \"/tmp/ipykernel_17020/1658758359.py\", line 18, in parameter_step  *\n        driver = inspector.run_model(\n\n    NameError: name 'input_video' is not defined\n"
     ]
    }
   ],
   "source": [
    "# model export\n",
    "@tf.function\n",
    "def distributed_run_step():\n",
    "    per_replica = mirrored_strategy.run(parameter_step)\n",
    "    return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM,per_replica,axis=None)\n",
    "results = distributed_run_step()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://hpkim0512.blogspot.com/2017/11/tensorflow-multi-gpu.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3b316d9bafbceebb8a7c74076820eeaf33b51a50a55a86f48ec9f9d66c01e89"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('ed': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
